{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0d8f1d-9dfe-4516-9c26-6661b61c7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_hums_data(folder_path):\n",
    "    data_list = []\n",
    "    filenames = sorted(glob.glob(os.path.join(folder_path, '*.mat')))\n",
    "    for file in filenames:\n",
    "        mat = loadmat(file)\n",
    "        if 'xr' in mat:\n",
    "            xr = mat['xr'].astype(np.float32).flatten()\n",
    "            data_list.append(xr)\n",
    "    return data_list, filenames\n",
    "\n",
    "train_seqs, train_files = load_hums_data('./data/AIRL/training_until20')\n",
    "test_seqs, test_files = load_hums_data('./data/AIRL/testAIRL_from21')\n",
    "\n",
    "# Normalize\n",
    "all_train = np.concatenate(train_seqs)\n",
    "mean_train, std_train = all_train.mean(), all_train.std()\n",
    "train_seqs = [(seq - mean_train) / (std_train + 1e-9) for seq in train_seqs]\n",
    "test_seqs = [(seq - mean_train) / (std_train + 1e-9) for seq in test_seqs]\n",
    "\n",
    "# Create transitions\n",
    "def create_transitions(seqs):\n",
    "    s, a, sp = [], [], []\n",
    "    for seq in seqs:\n",
    "        s.extend(seq[:-1])\n",
    "        a.extend(seq[1:])\n",
    "        sp.extend(seq[1:])\n",
    "    return torch.tensor(s).unsqueeze(-1).to(device), torch.tensor(a).unsqueeze(-1).to(device), torch.tensor(sp).unsqueeze(-1).to(device)\n",
    "\n",
    "\n",
    "\n",
    "s_train, a_train, sp_train = create_transitions(train_seqs)\n",
    "\n",
    "\n",
    "def save_scores(fname, file_list, score_list):\n",
    "    arr = np.empty(len(file_list), dtype=object)   \n",
    "    arr[:] = list(zip(file_list, score_list))\n",
    "    np.save(fname, arr, allow_pickle=True)         \n",
    "\n",
    "\n",
    "class RewardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(2,64),nn.ReLU(),nn.Linear(64,1))\n",
    "    def forward(self, s,a):\n",
    "        return self.net(torch.cat([s,a], dim=1))\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(1,64),nn.ReLU(),nn.Linear(64,1))\n",
    "    def forward(self,s):\n",
    "        return self.net(s)\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(1,64), nn.ReLU())\n",
    "        self.mean = nn.Linear(64,1)\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.ones(1) * -1.0)\n",
    "    def forward(self, s):\n",
    "        x = self.fc(s)\n",
    "        std = torch.clamp(self.log_std.exp(), min=1e-3)   # avoid exact zero\n",
    "        return self.mean(x), std\n",
    "\n",
    "    def sample(self, s):\n",
    "        mu, std = self.forward(s)\n",
    "        dist = Normal(mu, std)\n",
    "        a = dist.rsample()\n",
    "        logp = dist.log_prob(a)\n",
    "        return a, logp\n",
    "\n",
    "# Instantiate models\n",
    "reward_net = RewardNet().to(device)\n",
    "value_net = ValueNet().to(device)\n",
    "policy_net = PolicyNet().to(device)\n",
    "\n",
    "opt_disc = optim.Adam(list(reward_net.parameters())+list(value_net.parameters()),lr=1e-3)\n",
    "opt_policy = optim.Adam(policy_net.parameters(),lr=1e-3)\n",
    "\n",
    "gamma             = 0.99\n",
    "beta              = 0.01       \n",
    "batch_size        = 128\n",
    "episodes_per_epoch= 50\n",
    "steps_per_episode = 50\n",
    "num_epochs        = 300\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Discriminator (AIRL)\n",
    "    idx     = torch.randint(0, len(s_train), (batch_size,))\n",
    "    s_exp, a_exp, sp_exp = s_train[idx], a_train[idx], sp_train[idx]\n",
    "    s_fake, _ = s_train[idx], None\n",
    "    a_fake, _ = policy_net.sample(s_fake)\n",
    "    sp_fake   = a_fake\n",
    "\n",
    "    r_exp     = reward_net(s_exp, a_exp)\n",
    "    r_fake    = reward_net(s_fake, a_fake)\n",
    "    V_se, V_spe = value_net(s_exp),  value_net(sp_exp)\n",
    "    V_sf, V_spf = value_net(s_fake),  value_net(sp_fake)\n",
    "\n",
    "    D_exp     = torch.sigmoid(r_exp + gamma*V_spe - V_se)\n",
    "    D_fake    = torch.sigmoid(r_fake + gamma*V_spf - V_sf)\n",
    "\n",
    "    loss_disc = - (torch.log(D_exp + 1e-9).mean()\n",
    "                  + torch.log(1 - D_fake + 1e-9).mean())\n",
    "    opt_disc.zero_grad()\n",
    "    loss_disc.backward()\n",
    "    opt_disc.step()\n",
    "\n",
    "    # Policy\n",
    "    policy_loss_total = 0.0\n",
    "    for ep in range(episodes_per_epoch):\n",
    "        idx = torch.randint(0, len(s_train), (1,))\n",
    "        s   = s_train[idx]\n",
    "\n",
    "        logps, rewards, entropies = [], [], []\n",
    "        for step in range(steps_per_episode):\n",
    "            a, logp = policy_net.sample(s)\n",
    "            mu, std = policy_net.forward(s)\n",
    "            dist     = Normal(mu, std)\n",
    "\n",
    "            entropies.append(dist.entropy().mean())\n",
    "\n",
    "            sp = a \n",
    "            with torch.no_grad():\n",
    "                r = reward_net(s, a).detach()\n",
    "\n",
    "            logps.append(logp)\n",
    "            rewards.append(r)\n",
    "  \n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + gamma * G\n",
    "            returns.insert(0, G)\n",
    "        returns = torch.stack(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "\n",
    "        logps = torch.stack(logps).squeeze()   # shape [steps]\n",
    "        ent   = torch.stack(entropies).mean()\n",
    "\n",
    "        policy_loss_ep = -(logps * returns.detach()).mean() - beta * ent\n",
    "        policy_loss_total += policy_loss_ep\n",
    "\n",
    "\n",
    "    policy_loss_total /= episodes_per_epoch\n",
    "    opt_policy.zero_grad()\n",
    "    policy_loss_total.backward()\n",
    "    opt_policy.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d}  \"\n",
    "            f\"Disc Loss={loss_disc:.4f}  \"\n",
    "            f\"log_std={policy_net.log_std.item():+.3f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4cf6ef-c31b-453b-bc49-64bf97489ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = {}\n",
    "with torch.no_grad():\n",
    "    D_train = torch.sigmoid(reward_net(s_train, a_train) + gamma * value_net(sp_train) - value_net(s_train))\n",
    "    anom_train = (1 - D_train).cpu().numpy()\n",
    "\n",
    "    file_ptr = 0\n",
    "    file_level_scores = []\n",
    "    for seq, fname in zip(train_seqs, train_files):\n",
    "        n = len(seq) - 1\n",
    "        score = anom_train[file_ptr : file_ptr + n].max()  # or .mean()\n",
    "        file_level_scores.append(score)\n",
    "        file_ptr += n\n",
    "\n",
    "    save_scores(\n",
    "        \"train_anomaly_scores_sensorRF2.npy\",\n",
    "        train_files,\n",
    "        file_level_scores\n",
    "    )\n",
    "    \n",
    "    thresholds['max'] = anom_train.max()\n",
    "    std_error = anom_train.std() / np.sqrt(len(anom_train))\n",
    "    thresholds['max_minus_stderr'] = anom_train.max() - std_error\n",
    "    n_samples = len(anom_train)\n",
    "    std_error = anom_train.std() / np.sqrt(n_samples)\n",
    "    thresholds['max_minus_2sterror'] = anom_train.max() - 2*std_error\n",
    "\n",
    "    thresholds['manual_075'] = 0.78\n",
    "\n",
    "test_scores = []\n",
    "for seq, fname in zip(test_seqs, test_files):\n",
    "    s, a, sp = create_transitions([seq])\n",
    "    with torch.no_grad():\n",
    "        D_test = torch.sigmoid(reward_net(s, a) + gamma * value_net(sp) - value_net(s))\n",
    "        anom_score = (1 - D_test).max().item()\n",
    "    test_scores.append((fname, anom_score))\n",
    "\n",
    "save_scores(\"test_anomaly_scores_sensorRF2.npy\",\n",
    "            [f for f, _ in test_scores],\n",
    "            [s for _, s in test_scores])\n",
    "\n",
    "for method, thresh in thresholds.items():\n",
    "    print(f\"Threshold ({method}): {thresh:.4f}\")\n",
    "    anomalous_files = [f for f, s in test_scores if s > thresh]\n",
    "    if anomalous_files:\n",
    "        print(\"Earliest Anomalous File:\", anomalous_files[0])\n",
    "    else:\n",
    "        print(\"No anomalies detected.\")\n",
    "\n",
    "ranked = sorted(test_scores, key=lambda x: x[1], reverse=True)\n",
    "for fname, score in ranked[:]:\n",
    "    print(f\"{fname}: {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
